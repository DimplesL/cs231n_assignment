import numpy as np
import matplotlib.pyplot as plt
from cs231n.classifier_trainer import ClassifierTrainer
from cs231n.gradient_check import eval_numerical_gradient
from cs231n.classifiers.convnet import *
from cs231n.data_utils import load_CIFAR10

def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):
    """
    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare
    it for the two-layer neural net classifier. These are the same steps as
    we used for the SVM, but condensed to a single function.  
    """
    # Load the raw CIFAR-10 data
    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'
    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)
        
    # Subsample the data
    mask = range(num_training, num_training + num_validation)
    X_val = X_train[mask]
    y_val = y_train[mask]
    mask = range(num_training)
    X_train = X_train[mask]
    y_train = y_train[mask]
    mask = range(num_test)
    X_test = X_test[mask]
    y_test = y_test[mask]

    # Normalize the data: subtract the mean image
    mean_image = np.mean(X_train, axis=0)
    X_train -= mean_image
    X_val -= mean_image
    X_test -= mean_image
    
    # Transpose so that channels come first
    X_train = X_train.transpose(0, 3, 1, 2).copy()
    X_val = X_val.transpose(0, 3, 1, 2).copy()
    x_test = X_test.transpose(0, 3, 1, 2).copy()

    return X_train, y_train, X_val, y_val, X_test, y_test


# Invoke the above function to get our data.
X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()
print X_train.shape
print X_val.shape
print X_test.shape


def sanity_check_loss():
    model = init_five_layer_convnet()
    X = np.random.randn(100,3,32,32)
    y = np.random.randint(10,size=100)
    loss, _ = five_layer_convnet(X,model,y,reg=0)
    print 'Sanity check loss no regularization: ',loss
    loss, _ = five_layer_convnet(X,model,y,reg=0.5)
    print 'Sanity check loss with reg:', loss

#sanity_check_loss()
#model = init_five_layer_convnet()
#X = np.random.randn(100,3,32,32)
#y = np.random.randint(10,size=100)
#loss, _ = five_layer_convnet(X,model,y,reg=0)
#print 'Sanity check loss no regularization: ',loss
#loss, _ = five_layer_convnet(X,model,y,reg=0.5)
#print 'Sanity check loss with reg:', loss

num_inputs = 2
input_shape = (3,16,16)
model = init_five_layer_convnet(input_shape=input_shape)
reg = 0.0
num_classes = 10
X = np.random.randn(num_inputs, *input_shape)
y = np.random.randint(num_classes, size=num_inputs)
loss, grads = five_layer_convnet(X, model, y)
for param_name in sorted(grads):
    f = lambda _: five_layer_convnet(X, model, y)[0]
    param_grad_num = eval_numerical_gradient(f, model[param_name], verbose=False, h=1e-6)
    e = rel_error(param_grad_num, grads[param_name])
    print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))
